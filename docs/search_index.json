[["index.html", "數據科學隨筆 Introduction Side Story 0.1 TBrain 0.2 Kaggle 0.3 KAUST", " 數據科學隨筆 DongDong 2022-05-08 Introduction Hi, 我是 DongDong，我會不定期的分享一些我在學習過程中的隨筆，詳情請見我的個人網頁 DongDong Promise me you’ll always remember: You’re braver than you believe, and stronger than you seem, and smarter than you think. ~ Christopher Robin to Pooh – A.A.Milne ~ Side Story 人們總是覺得自己不夠強大、做不到、無法完成挑戰，但事實上並不是這樣的。我見過形形色色的人，有的天賦異稟卻疲於求知，有的不夠自信、總怯於邁出第一步，其實不逼一逼自己就不知道自己原來這麼厲害。 ps 如果你只對資料科學有興趣，可以省略這個區塊，但如果你是想來喝點下午茶，不彷來這裡逛逛 Smarter than you think 我大學期間的興趣就是做完份內工作後打一場 LOL 來慰勞自己一整天的辛勞。某一天，我像往常一樣，打開 LOL 玩了一小會，在某一場遊戲中，我跟隊友出現了點摩擦，這本來是一件稀鬆平常的事，直到我的隊友在遊戲的聊天室打出了這麼一句話，xxx 出來打球啊!，我當下的第一想法是，這個人我認識嗎? 後來轉念一想，不對，他怎麼知道我的真實姓名的? 我當下是非常震驚的，為了搞清楚他怎麼得到我的姓名，我就上網到處查一些前端的知識，並且用這個知識也查到對方的真實姓名還有生日。我不是鼓勵大家去肉搜別人，只是你永遠不要小看自己，即使像我一樣完全沒學過編程，也可以像我一樣不懈努力的去學習。 來自一個個資被外洩的普通玩家 ~~ DongDong ps 每個平台的帳號盡量都用不同的，以免被有心人士調查 真正意義上的變強 如果你不知道你寫的答案是不是對的，那就是錯的 ~~ 陳志偉教授 囫圇吞棗的學習不會使自己進步，相反地，不僅浪費時間，久而久之趨於現狀而不知進取。念書不是把書本讀過一遍就說自己好棒棒，應該做的是去思考書中內容各個觀念之間的關聯性，這是一種訓練邏輯組織的方式，這就是為什麼有人舉一反三，有人卻只會寫題庫裡的題目，這是填鴨式教育的失敗，不過請別灰心，這是可以透過後天改變的，開始學著不要帶入先入為主的觀念吧，開始學著懷疑一切，沒有得到合理解釋前別輕易放棄你的求知慾，但也並非要你病態的追求證明，而是必須在每個現象都給出一個合理且可以說服你的解釋，下面有幾個小故事可以玩玩到。 ps 大家可能不想看無聊的陳述，我們直接看看故事吧!! ICU 驚魂 一天，你因為一個小車禍進了醫院。在醫院辦理手續時你看到護士和醫生們推著一個病床急沖沖的衝向手術室，病床上躺著一個不醒人事的年輕小女孩。你聽到旁邊的護士說是小女孩因為車禍導致昏迷，除此之外你注意到在病床旁邊有個中年男子憂心忡忡的說道，「爸爸和醫生一定會救你的」，這時，戲劇性的一幕出現了，手術室裡的外科主任正好從 ICU 走出來，看了看病床上的病人，驚呼道 「這不是我的女兒嗎 ?」。仔細思考，你是否覺得這個場景很不合理? 點擊查看解答 如果你覺得不合理是因為你帶有先入為主的概念，在人們的潛意識中，我們認定外科主任就一定是男性，所以我們會覺得，難道小女孩的爸媽都是男生嗎? 又或者其中一個人認錯了自己的女兒，但其實都不然，正解是外科主任就是小女孩的媽媽，一位事業成功的女性。 股市騙局 連續十個禮拜，沒錯，連續十個禮拜你收到了一家股市公司預測 Tesla 股價在下周是漲或是跌，這家股市公司已經連續十個禮拜預測了 Tesla 股市在每個禮拜的漲跌，每次都預測正確。太厲害了吧，我要學戴夫跟著這家股市公司做預測，看到這裡你是否心動了呢? 不過很遺憾，正確預測股市行情的事情是絕對不會發生的，會連續十個禮拜正確是因為他們的工作人員都是高中畢業，學過排列組合。 點擊查看解答 每個禮拜都有漲跌兩種情況，連續十個禮拜就有 1024 種可能的排列組合，換句話說，這家股市公司只要將這十個禮拜 1024 種可能寄給 1024 個人，每個人收到的信件內容都不一樣，則根據雞兔同籠的原理，必然有一個倒楣鬼收到的預測是完全正確的。 0.1 TBrain 參加過的 TBrain 競賽 : 蘭花種類辨識 此區域仍然在施工! 0.2 Kaggle 參加過的 Kaggle 競賽 : Tabular Palyground 2021/08 Tabular Playground 2021/09 此區域仍然在施工! 0.3 KAUST 我參加了 KAUST 大學舉辦的空間資料大數據預測競賽，以下是 Kaggle 成績 1a: Top 3 2a: Top 1 2b: Top 1 此區域仍然在施工! "],["introduction-to-kriging.html", "Chapter 1 Introduction to Kriging 1.1 Assumption 1.2 BLUE (Best Linear Unbiased Estimator) 1.3 R 語言實作", " Chapter 1 Introduction to Kriging 克里金法(Kriging)是一種用於空間資料的插值方法，通常視為高斯過程回歸或者 BLUP。 1.1 Assumption 固有平穩過程 (intrinsically stationary process) 數學期望與地理位置無關，即觀察任一小區域他的屬性應當相同 (constant mean or variance) 半方差函數只與距離有關，即觀察任一小區域他的 Variogram 應當相同 (constant variogram) 若未滿足假設(可由 Voronoi 圖檢驗)，可以對資料做轉換 (log transformation of squared) Pros and cons Pros : kriging model 告訴我們感興趣的空間特徵均值，且是最佳無偏 (BLUP)，並且可以估計誤差範圍 Cons : 每當你要重新估計其他區域的資料，你的線性系統要被完全更新 Relation to GPR GPR : 隨機場為高斯過程 Kriging : 隨機場為固有平穩過程 若協方差函數相同，則普通克里金與簡單克里金法與GPR利用正態分配的最大似然估計輸出相同 1.2 BLUE (Best Linear Unbiased Estimator) 通常我們稱克里金法為最佳無偏線性預測(BLUP) : * Best : 最小變方 (最小方差) * Linear : 估計值為觀測值的線性組合 * Unbiased : 估計值的期望值等於真值 1.2.1 普通克里金 (OK Oridnary Kriging) 我們相信空間資料與地理位置具有相關性，對於未知的某個空間資訊\\(Y_0\\)，我們透過觀測其附近 \\(n\\) 個位置 \\(s_1,\\cdots,s_n\\) 的 \\(n\\) 個空間資訊 \\(Y\\) 來線性組合估計 \\(Y_0\\)，及以下表述 : \\[ \\hat Y(s_0) = \\sum_{i=1}^n\\lambda_iY(s_i) \\tag 1 \\] 現在我們只要找出權重係數 \\(\\lambda_i\\) 就能解決問題了!，為了滿足無偏的性質，普通克里金(Oridnary Kriging) 假設空間資訊在每一個地理位置都有相同的相同期望值 (constant unknown mean)，即數學期望值與空間位置無關 \\[ Y(\\mathbf{s}) = \\mu + \\delta(\\mathbf{s})\\tag 2 \\] 其中 \\(E(Y) = \\mu\\) 且 \\(\\delta\\) (平均為0的固有平穩過程) 為地理位置 \\(\\mathbf{s}\\)，\\(Y\\) 與 \\(\\hat Y\\) 的誤差 \\[ E(\\hat Y(s_0)) = \\mu = \\mu\\sum\\lambda_i \\iff \\sum \\lambda_i = 1 \\tag 3 \\] 為了滿足在限制條件 \\((3)\\) 下最佳化 \\[ \\arg\\min_\\lambda E(||\\hat Y(s_0) - Y(s_0)||^2) \\iff \\arg\\min_\\lambda \\frac{1}{2}E(||\\sum_{i=1}^n\\lambda_iY(s_i) -Y(s_0)||^2) \\\\ \\mbox{subject to } \\sum_{i=1}^n\\lambda_i = 1 \\tag 4 \\] 利用拉格朗日乘數 \\(L(\\lambda_1,\\cdots,\\lambda_n,\\phi)\\) 可得，詳見 克里金法原理與推導 \\[ \\begin{bmatrix} r_{11}&amp;r_{12}&amp;\\cdots&amp;r_{1n}&amp;1\\\\ r_{21}&amp;r_{22}&amp;\\cdots&amp;r_{2n}&amp;1\\\\ \\cdots&amp;\\cdots&amp;\\cdots&amp;\\cdots&amp;\\cdots\\\\ r_{n1}&amp;r_{n2}&amp;\\cdots&amp;r_{nn}&amp;1\\\\1&amp;1&amp;\\cdots&amp;1&amp;0\\end{bmatrix} \\begin{bmatrix} \\lambda_1\\\\ \\lambda_2\\\\\\cdots\\\\\\lambda_n\\\\-\\phi\\end{bmatrix}=\\begin{bmatrix} r_{1o}\\\\ r_{2o}\\\\\\cdots\\\\r_{no}\\\\1\\end{bmatrix} \\tag 5 \\] 其中 \\(r_{ij}\\) 為半方差函數，\\(\\phi\\) 為拉格朗日的限制參數，做完逆矩陣即可得權重係數 \\[ \\begin{bmatrix} \\lambda_1\\\\ \\lambda_2\\\\\\cdots\\\\\\lambda_n\\\\-\\phi\\end{bmatrix}= \\begin{bmatrix} r_{11}&amp;r_{12}&amp;\\cdots&amp;r_{1n}&amp;1\\\\ r_{21}&amp;r_{22}&amp;\\cdots&amp;r_{2n}&amp;1\\\\ \\cdots&amp;\\cdots&amp;\\cdots&amp;\\cdots&amp;\\cdots\\\\ r_{n1}&amp;r_{n2}&amp;\\cdots&amp;r_{nn}&amp;1\\\\1&amp;1&amp;\\cdots&amp;1&amp;0\\end{bmatrix}^{-1} \\begin{bmatrix} r_{1o}\\\\ r_{2o}\\\\\\cdots\\\\r_{no}\\\\1\\end{bmatrix} \\tag 6 \\] 我們計做 \\(\\lambda_O = \\Gamma_O^{-1}\\gamma_O\\)，帶回原式我們可以估得誤差的方差為 \\(\\sigma^2_O =\\lambda_O^\\top\\gamma_O= \\gamma_O^\\top \\Gamma_O^{-1}\\gamma_O\\) 1.2.2 半方差圖 (Variogram) 半方差的定義為 \\(r_{ij} = \\frac{1}{2}E[(Y(s_i) - Y(s_j))^2]\\)，根據我們的假設，我們希望地理位置鄰近的地方具有相似的空間資訊，即 \\(\\mbox{Distance}\\rightarrow 0 \\implies r_{ij} \\rightarrow 0\\) 半方差圖其中幾個重要的術語 Data : 即 \\(\\frac{1}{2}(Y(s_i) - Y(s_j))^2\\)，為上圖中紅色標記點 Variogram model : 即是 Data 的擬合曲線，常用“線性”、“高斯”、“球面”、“指數”等逼近 Nugget : 用 \\(C_o\\) 表示，為距離最近兩點的半方差值 越大的 \\(C_o\\) 表示空間測量誤差越大 Sill : 用 \\(C_o + C\\) 表示，當半方差函數達到一個穩定態時，該半方差值稱為 Sill Range : 用 \\(A_o\\) 表示，達到 Sill 的距離，表示空間相關性的作用範圍 超過 \\(A_o\\) 我們稱空間相關性不存在，即不能使用內插法 1.2.3 簡單克里金 (SK Simple Kriging) 不同於普通克里金法，簡單克里金假設未知點的偏差是由已知點的偏差線性組合(with known mean) \\[ \\hat Y(s_0) - \\mu = \\sum_{i=1}^n\\lambda_i(Y(s_i) - \\mu) \\] 神奇的是，這個條件會自動滿足無偏性 \\[ E(\\hat Y_0) = E(\\mu + \\sum_{i=1}^n\\lambda_i(Y(s_i) - \\mu)) = \\mu \\] 所以它不需要普通克里金法的限制 \\((3)\\) 1.2.4 泛克里金 (UK Universal kriging) 泛克里金法假設隨機場 \\(Y\\) 是由隨機場的漂移與固有平穩過程生成出來的 \\[ Y(\\mathbf{s}) = \\sum_{i=1}^p\\beta_if_i(\\mathbf{s}) + \\delta(\\mathbf{s}) \\] 注意到如果取 \\(f_i = 1,p=1\\) 則與普通克里金一樣，為了簡便說明，給出以下符號 \\[ \\mathbf{X} = \\begin{bmatrix}f_1(s_1)&amp;\\cdots&amp;f_p(s_1)\\\\f_1(s_2)&amp;\\cdots&amp;f_p(s_2)\\\\\\vdots&amp;\\cdots&amp;\\vdots\\\\f_1(s_n)&amp;\\cdots&amp;f_p(s_n)\\end{bmatrix},\\mathbf{\\beta} = \\begin{bmatrix}\\beta_1\\\\\\vdots\\\\\\beta_p\\end{bmatrix} \\\\ \\mathbf{\\delta(s)} = \\begin{bmatrix}\\delta(s_1)\\\\\\vdots\\\\\\delta(s_n)\\end{bmatrix}, \\mathbf{Y} = \\begin{bmatrix}Y(s_1)\\\\\\vdots\\\\Y(s_n)\\end{bmatrix} \\] 則有 \\(\\bf Y = X\\beta + \\delta\\)，同樣地，為了滿足無偏性，我們有以下限制條件 \\[ E(Y) = \\sum_{i=1}^p\\beta_if_i(\\mathbf{s}) \\\\ E(\\hat Y(s_0)) = E(\\sum_{i=1}^n\\lambda_iY(s_i)) = \\sum_{i=1}^n\\lambda_i\\sum_{j=1}^p\\beta_jf_j(s_i) = \\sum_{i=1}^p\\beta_if_i(\\mathbf{s_0}) \\\\ f_j(\\mathbf{s}) = \\sum_{i=1}^n\\lambda_if_j(s_i) \\mbox{ for } j = 1,\\cdots,p \\\\ \\sum_{i=1}^n\\lambda_i = 1 \\] 再根據拉格朗日乘數求解方程，詳見 克里金法原理與推導 \\[ L(\\lambda_1,\\cdots,\\lambda_n,m_1,\\cdots,m_p) = E[||\\hat Y(s_0) - Y(s_0)||^2] - 2m_0(\\sum_{i=1}^n\\lambda_i-1) - 2\\sum_{j=1}^pm_j(\\sum_{i=1}^n\\lambda_if_j(s_i) - f_j(s_0)) \\] 即有 \\[ \\begin{bmatrix} \\lambda_1 \\\\ \\vdots \\\\ \\lambda_n \\\\ m_0 \\\\ m_1 \\\\ \\vdots \\\\ m_p \\end{bmatrix} = \\begin{bmatrix} r_{11} &amp; \\cdots &amp; r_{1n} &amp;1&amp; f_1(s_1) &amp; \\cdots &amp; f_1(x_n)\\\\ \\vdots &amp; \\ddots &amp; \\vdots &amp;\\vdots&amp; \\vdots &amp; \\cdots &amp; \\vdots\\\\ r_{n1} &amp; \\cdots &amp; r_{nn} &amp;1&amp; f_p(s_1) &amp; \\cdots &amp; f_p(s_n)\\\\ 1&amp;\\cdots&amp;1&amp;0&amp;0&amp;\\cdots&amp;0 \\\\ f_1(s_1) &amp; \\cdots &amp; f_1(s_n) &amp;0&amp; 0 &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\ddots &amp; \\vdots &amp;\\vdots&amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ f_p(s_1) &amp; \\cdots &amp; f_p(s_n) &amp; 0 &amp; 0 &amp; \\cdots &amp;0 \\end{bmatrix}^{-1}\\begin{bmatrix} r_{10} \\\\ \\vdots \\\\ r_{n0} \\\\ 1 \\\\ f_1(s_0) \\\\ \\vdots \\\\ f_p(s_0) \\end{bmatrix} \\] 我們計做 \\(\\lambda_U = \\Gamma_U^{-1}\\gamma_U\\)，帶回原式我們可以估得誤差的方差為 \\(\\sigma^2_U =\\lambda_U^\\top\\gamma_U= \\gamma_U^\\top \\Gamma_U^{-1}\\gamma_U\\) 更進一步，如果 \\(Y\\) 的協方差矩陣 \\(\\Sigma\\) 已知，則根據廣義最小平方法，將空間資訊 \\(Y\\) 投影到主成分空間使得誤差的斜方差矩陣沒有外部協方差，則有 \\[ \\hat\\beta_{gls} = (X^\\top\\Sigma^{-1} X)^{-1}X^\\top\\Sigma^{-1} Y \\] 1.2.5 協同克里金 (CK Co-Kriging) 協同克里金是克里金法在多變量分析的衍生，協同克里金允許多個空間資訊 \\(Y_1,\\cdots,Y_K\\) 分別在地理位置 \\(\\{s_1,\\cdots,s_{n_k}\\}, k=1,\\cdots,K\\) 被觀測到，則 \\[ \\hat Y(s_0) = \\sum_{k=1}^K\\sum_{i=1}^{n_k}\\lambda_{k_i}Y_k(s_i) \\] 1.2.6 析取克里金 (DK Disjunctive Kriging) 析取克里金是隨機場的非線性估計，注意，在此之前都是線性估計!! \\[ \\hat Y(s_0) = \\sum_{i=1}^nf_i(Y(s_i)) \\] 如果 \\(f_i\\) 為指示函數，則析取克里金又稱為指示克里金 (IK Indicator Kriging) 1.3 R 語言實作 選定資料集與載入套件 library(gstat) # for kriging library(sp) # for meuse dataset library(FRK) # for gird and fixed rank kriging library(tidyverse) # for data flow library(gridExtra) # for combine plot data(meuse) 將資料轉成 SP 格式 coordinates(meuse) = ~ x + y 建立要預測地點的網格 GridBAUs &lt;- auto_BAUs(manifold = plane(), # 2D plane cellsize = c(100,100), # BAU cellsize type = &quot;grid&quot;, # grid (not hex) data = meuse, # data around which to create BAUs convex=-0.05, # border buffer factor nonconvex_hull=FALSE) # convex hull 1.3.0.1 反距離權重插值法 idw2 &lt;- idw(formula = log(zinc) ~ 1, # formula locations = meuse, # data with measurements newdata = GridBAUs, # where to be interpolated nmin = 0, # the least numbers of data to be interpolated, nmax = Inf, # the largest numbers of data to be interpolated, maxdist = Inf, # the maximum distance of datat to be interpolated idp = 2, # the distance attenuation parameter ) ## [inverse distance weighted interpolation] 建立半方差模型 lzn.vgm &lt;- variogram(log(zinc)~1, meuse) # calculates sample variogram values lzn.model &lt;- vgm(psill = 1, # create model model = &quot;Sph&quot;, # &quot;Sph&quot;, &quot;Exp&quot;, &quot;Gau&quot;, &quot;Mat&quot;, &quot;Pow&quot; range = 900, nugget = 1) lzn.fit &lt;- fit.variogram(lzn.vgm, lzn.model) # fit model plot(lzn.vgm, lzn.fit) 1.3.0.2 建立各種克里金模型 OK &lt;- krige(log(zinc) ~ 1, meuse, GridBAUs, model=lzn.fit) # Ordinary kriging ## [using ordinary kriging] UK &lt;- krige(log(zinc) ~ x+y, meuse, GridBAUs, model = lzn.fit) # Universal kriging ## [using universal kriging] lm(var1.pred ~ x + y, UK) # Use Universal kriging to calaulate the coefficient ## ## Call: ## lm(formula = var1.pred ~ x + y, data = UK) ## ## Coefficients: ## (Intercept) x y ## -8.0324313 -0.0008891 0.0005250 SK &lt;- krige(log(zinc)~x+y, meuse, GridBAUs, lzn.fit, beta = c(-8.0324313, -0.0008891, 0.0005250)) # Simple kriging ## [using simple kriging] CK &lt;- gstat(NULL, &quot;logCd&quot;, log(cadmium) ~ 1, meuse) CK &lt;- gstat(CK, &quot;logCu&quot;, log(copper) ~ 1, meuse) CK &lt;- gstat(CK, &quot;logPb&quot;, log(lead) ~ 1,meuse) CK &lt;- gstat(CK, &quot;logZn&quot;, log(zinc) ~ 1,meuse) CK.vgm &lt;- variogram(CK) CK.model &lt;- vgm(psill = 1, model = &quot;Sph&quot;, range = 800, nugget = 1) CK.fit &lt;- fit.lmc(CK.vgm, CK, CK.model) plot(CK.vgm, CK.fit) CK &lt;- predict(CK.fit, GridBAUs) # Ordinary CoKriging ## Linear Model of Coregionalization found. Good. ## [using ordinary cokriging] 各種模型比較 OK.plot &lt;- OK %&gt;% as.data.frame %&gt;% ggplot() + aes(x,y,fill = var1.pred) + geom_tile() + coord_fixed() + theme_bw() + labs(title = &#39;Ordinary Kriging&#39;) + scale_fill_distiller(palette=&quot;Spectral&quot;, name=&quot;pred.&quot;) + geom_point(data=data.frame(meuse), aes(x,y,fill=log(zinc)), colour=&quot;black&quot;, pch=21, size=3) UK.plot &lt;- UK %&gt;% as.data.frame %&gt;% ggplot() + aes(x,y,fill = var1.pred) + geom_tile() + coord_fixed() + theme_bw() + labs(title = &#39;Universal Kriging&#39;) + scale_fill_distiller(palette=&quot;Spectral&quot;, name=&quot;pred.&quot;) + geom_point(data=data.frame(meuse), aes(x,y,fill=log(zinc)), colour=&quot;black&quot;, pch=21, size=3) SK.plot &lt;- SK %&gt;% as.data.frame %&gt;% ggplot() + aes(x,y,fill = var1.pred) + geom_tile() + coord_fixed() + theme_bw() + labs(title = &#39;Simple Kriging&#39;) + scale_fill_distiller(palette=&quot;Spectral&quot;, name=&quot;pred.&quot;) + geom_point(data=data.frame(meuse), aes(x,y,fill=log(zinc)), colour=&quot;black&quot;, pch=21, size=3) CK.plot &lt;- CK %&gt;% as.data.frame %&gt;% ggplot() + aes(x,y,fill = logZn.pred) + geom_tile() + coord_fixed() + theme_bw() + labs(title = &#39;CoKriging&#39;) + scale_fill_distiller(palette=&quot;Spectral&quot;, name=&quot;pred.&quot;) + geom_point(data=data.frame(meuse), aes(x,y,fill=log(zinc)), colour=&quot;black&quot;, pch=21, size=3) grid.arrange(SK.plot, OK.plot, UK.plot, CK.plot, ncol = 2) 比較各種模型誤差 OK.varplot &lt;- OK %&gt;% as.data.frame %&gt;% ggplot() + aes(x,y,fill = var1.var) + geom_tile() + coord_fixed() + theme_bw() + labs(title = &#39;Ordinary Kriging&#39;) + scale_fill_distiller(palette=&quot;Spectral&quot;, name=&quot;var.&quot;) UK.varplot &lt;- UK %&gt;% as.data.frame %&gt;% ggplot() + aes(x,y,fill = var1.var) + geom_tile() + coord_fixed() + theme_bw() + labs(title = &#39;Universal Kriging&#39;) + scale_fill_distiller(palette=&quot;Spectral&quot;, name=&quot;var.&quot;) SK.varplot &lt;- SK %&gt;% as.data.frame %&gt;% ggplot() + aes(x,y,fill = var1.var) + geom_tile() + coord_fixed() + theme_bw() + labs(title = &#39;Simple Kriging&#39;) + scale_fill_distiller(palette=&quot;Spectral&quot;, name=&quot;var.&quot;) CK.varplot &lt;- CK %&gt;% as.data.frame %&gt;% ggplot() + aes(x,y,fill = logZn.var) + geom_tile() + coord_fixed() + theme_bw() + labs(title = &#39;CoKriging&#39;) + scale_fill_distiller(palette=&quot;Spectral&quot;, name=&quot;var.&quot;) grid.arrange(SK.varplot, OK.varplot, UK.varplot, CK.varplot, ncol = 2) 克里金模型交叉驗證 OK.cv &lt;- krige.cv(log(zinc) ~ 1, meuse, lzn.fit, nfold = 10, verbose = F) UK.cv &lt;- krige.cv(log(zinc) ~ x + y, meuse, lzn.fit, nfold = 10, verbose = F) SK.cv &lt;- krige.cv(log(zinc) ~ x + y, meuse, lzn.fit, nfold = 10, verbose = F, beta = c(-8.0324313, -0.0008891, 0.0005250)) CK.cv &lt;- gstat.cv(CK.fit, verbose = F, all.residuals = T) "],["linear-algebra.html", "Chapter 2 Linear Algebra 2.1 Vector Space (向量空間) 2.2 Linear independent and Linear dependent (線性獨立與線性相依) 2.3 Basis and Space Property (基底與空間特性) 2.4 Linear transformation (線性映射) 2.5 Rank of a Matrix (矩陣的秩) 2.6 Direct Sum and Projection (直和與投影) 2.7 Change Basis and Coordinates Matrix (基底變換與基底變換矩陣) 2.8 線性變換矩陣表示法 2.9 Eigenvalue and Eigenvetor (特徵值與特徵向量) 2.10 Diagonalizable (可對角化) 2.11 Eigen Value Decomposition (特徵值分解)", " Chapter 2 Linear Algebra 2.1 Vector Space (向量空間) 令 \\(V\\) 為非空集合(元素為向量)，\\(F\\) 為體(元素為純量) 滿足以下八條規則稱 \\((V,+,\\cdot)\\) 為佈於 \\(F\\) 的向量空間 : 向量加法結合律 \\(\\forall\\) \\(u,v,w \\in V\\), \\((u + v) + w = u + (v + w)\\) 向量加法單位元素 \\(\\exists\\) \\(0 \\in V\\) such that \\(\\forall\\) \\(v \\in V\\), we have \\(v+0 = 0+v = v\\) 向量加法反元素 \\(\\forall\\) \\(v \\in V\\), \\(\\exists -v \\in V\\) such that \\(v + (-v) = (-v) + v = 0\\) 向量加法交換律 \\(\\forall\\) \\(u,v \\in V\\), \\(u+v = v+u\\) 純量乘法對向量加法有分配律 \\(\\forall\\) \\(\\alpha \\in F\\), \\(u,v \\in V\\), \\(\\alpha \\cdot(u+v) = \\alpha \\cdot u + \\alpha \\cdot v\\) 純量乘法對純量加法有分配律 \\(\\forall\\) \\(\\alpha, \\beta \\in F\\), \\(v \\in V\\), \\((\\alpha + \\beta) \\cdot v = \\alpha \\cdot v + \\beta \\cdot v\\) 純量乘法結合律 \\(\\forall\\) \\(\\alpha, \\beta \\in F\\), \\(v \\in V\\), \\((\\alpha\\beta) v = \\alpha(\\beta v)\\) 乘法單位元素 \\(\\forall v \\in V\\), \\(1 \\cdot v = v\\) Note : - 向量空間必有零向量(非空) 2.1.1 Subspacce (子空間) 令 \\(S\\) 為 \\(V\\) 的非空子集，且佈於同一 \\(F\\) 上，稱 \\((S,+, \\cdot)\\) 為 \\((V,+,\\cdot)\\) 的子空間 Note (令向量空間 \\(W\\) 為向量空間 \\(V\\) 的子空間) : \\(\\{0\\}\\), \\(V\\) 皆為向量空間 \\(V\\) 的顯然子空間(trivial subspace) 歐式空間的子空間必然通過原點 子空間有向量加法封閉性以及純量乘法封閉性 \\(\\forall\\) \\(u,v\\in W\\), \\(\\alpha \\in F\\), \\(\\alpha u +v \\in W\\) \\(0 \\in W\\) (常用來判斷是否為子空間) if \\(v\\in W\\), then \\(-v \\in W\\) (常用來判斷是否為子空間) 子空間交集仍為子空間 子空間聯集為子空間，若且唯若子空間之間有包含關係 2.1.2 Sum space (和空間) 假設 \\(W_1, \\cdots, W_k\\) 為 \\(V\\) 的子空間，則 \\(\\displaystyle \\sum_{i = 1}^k W_i := \\{\\sum_{i=1}^kv_i|v_i \\in W_i, 1 \\leq i \\leq k\\}\\) 稱為 \\(W_1, \\cdots, W_k\\) 的和空間 Note : 子空間的和空間仍為子空間 \\(W_1\\) \\(\\cup\\) \\(W_2\\) \\(\\subseteq\\) \\(W_1 + W_2\\) \\(W_1 + W_2\\) 是包含 \\(W_1\\) \\(\\cup\\) \\(W_2\\) 的最小子空間 2.1.3 Four Basis Spaces (四大基本子空間) 假設 \\(A \\in F^{m \\times n}\\) Row space 為 \\(A\\) row vector 的線性組合，\\(\\mbox{RS}(A) = \\{xA|x\\in F^{1 \\times m}\\}\\) Column space 為 \\(A\\) colume vector 的線性組合，\\(\\mbox{CS}(A) = \\{Ax|x\\in F^{m\\times 1}\\}\\) kernal space 為 \\(Ax = 0\\) 的 solution space，\\(\\mbox{ker}(A) = \\{x \\in F^{m \\times 1}|Ax = 0\\}\\) left kernal space 為 \\(xA = 0\\) 的 solution space，\\(L\\mbox{ker}(A) = \\{x \\in F^{1 \\times m}|xA = 0\\}\\) Note : \\(\\mbox{CS}(A) = \\mbox{Range}(A) = \\mbox{R}(A)\\) \\(\\mbox{ker}(A) = \\mbox{Nullspace}(A) = \\mbox{Null(A)} = \\mbox{N}(A)\\) \\(L\\mbox{ker}(A) = \\mbox{ker}(A^T)\\)，(\\(A\\) 的 row vector 為 \\(A^T\\) 的 column vector) 列運算不改變 Row space 和 kernal space 若\\(A\\) row equivalent to \\(B\\) 則 \\(\\mbox{RS}(A) = \\mbox{RS}(B)\\) 且 \\(\\mbox{ker}(A) = \\mbox{ker}(B)\\) 行運算不改變 Column space 和 left kernal space 若\\(A\\) cloumn equivalent to \\(B\\) 則 \\(\\mbox{CS}(A) = \\mbox{CS}(B)\\) 且 \\(L\\mbox{ker}(A) = L\\mbox{ker}(B)\\) \\(\\mbox{RS}(AB) \\subseteq \\mbox{RS}(B);\\) \\(\\mbox{CS}(AB) \\subseteq \\mbox{CS}(A)\\) 若 \\(AB = O\\) 則 \\(\\mbox{CS}(B)\\subseteq \\mbox{ker}(A);\\) \\(\\mbox{RS}(A) \\subseteq L\\mbox{ker}(B)\\) \\(Ax = b\\) is consistent 若且唯若 \\(b \\in \\mbox{CS}(A)\\) Clearly explaintion on feature 6. : \\[A_{m \\times n}B_{n \\times m} = \\begin{bmatrix} | &amp; \\vdots &amp; | \\\\ a_1 &amp; \\cdots &amp; a_n \\\\ | &amp; \\vdots &amp; | \\end{bmatrix}\\begin{bmatrix} b_{11} &amp; \\cdots &amp; b_{1m}\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ b_{n1} &amp; \\cdots &amp; b_{nm} \\end{bmatrix}\\] \\(AB\\) 可以看做是 \\(A\\) Column vector 的線性組合，顯而易見地 \\(AB\\) 的 Column space 自然會包含於 \\(A\\) 的 Column space \\[ A_{m\\times n}B_{n \\times m} = \\begin{bmatrix} a_{11} &amp; \\cdots &amp; a_{1n}\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{m1} &amp; \\cdots &amp; a_{mn} \\end{bmatrix}\\begin{bmatrix} － &amp; b_1 &amp; －\\\\ \\cdots &amp; \\vdots &amp; \\cdots\\\\ － &amp; b_n &amp;－ \\end{bmatrix} \\] \\(AB\\) 也可以看做是 \\(B\\) Row vector 的線性組合，顯而易見地 \\(AB\\) 的 Row space 自然會包含於 \\(B\\) 的 Row space Clearly explaintion on feature 7. : let \\(y \\in \\mbox{CS}(B)\\), i.e., \\(y = Bx\\) for some \\(x\\), then we have following relation \\(AB = O \\implies (AB)x = Ox = 0 \\iff A(Bx) = 0 \\iff Ay = 0 \\iff y \\in \\mbox{ker}(A)\\) let \\(y \\in \\mbox{RS}(A)\\), i.e., \\(y = xA\\) for some \\(x\\), then we have following relation \\(AB = O \\implies x(AB) = xO = 0 \\iff (xA)B = 0 \\iff yB = 0 \\iff y \\in L\\mbox{ker}(B)\\) 2.1.4 Solve Linear System (求解線性系統) 令 \\(A \\in F^{m \\times n}\\) 且 \\(x, b \\in F^{1 \\times n}\\) 線性系統 \\(Ax = b\\) 的其中一解稱為特解 (particular solution) 線性系統 \\(Ax = 0\\) 的解稱為零解 (homogeneous solution) 線性系統 \\(Ax = b\\) 的解為一個特解加上零解 A linear system \\(Ax = b\\) is called consistent if \\(b \\in \\mbox{CS}(A)\\) A linear system \\(Ax = b\\) is called inconsistent if \\(b \\notin \\mbox{CS}(A)\\) 2.1.5 線性系統求解演算法 \\(\\mbox{Input :}\\) an consistent matrix \\(A_{m \\times n}\\) and vector \\(b\\) which is in the column space of \\(A\\) Do row operation to obtain the echlon form \\((A\\mid b) \\rightarrow (R \\mid r)\\) Find all the free variables \\(y_1, \\cdots, y_k\\) Set \\(y_1= \\cdots = y_k = 0\\), solve \\(Rx = r\\) and get a particular solution \\(p\\) For each \\(i = 1, \\cdots, k\\) set \\(y_i = 1\\) and other \\(y = 0\\), solve \\(Rx = 0\\) and get a homogeneous solution \\(\\beta_i\\) \\(\\mbox{Output :}\\) \\(p + c_1\\beta_1 + \\cdots + c_k\\beta_k\\) 2.1.6 Spanning Set and Generating Set (生成集與獨立集) 令 \\(S\\) 為向量空間 \\(V_F\\) 的子集合，\\(\\displaystyle span(S) = \\{\\sum_{i = 1}^{k}\\alpha_i v_i \\mid v_i \\in S, \\alpha_i \\in F, 1 \\leq i \\leq k\\}\\) 稱為集合 \\(S\\) 的生成空間而 \\(S\\) 為\\(span(S)\\) 的生成集 Note (令 \\(S,S_1,S_2\\) 為向量空間 \\(V_F\\) 的子集合) : 我們約定 \\(span(\\phi) = \\{0\\}\\) \\(span(S)\\) 滿足加法與存量乘法封閉性，為包含 \\(S\\) 的最小子空間 \\(\\mbox{RS}(A) = span(\\{\\mbox{all the row vector in A}\\})\\) \\(\\mbox{CS}(A) = span(\\{\\mbox{all the column vector in A}\\})\\) \\(S \\subseteq span(S)\\) 且若 \\(S_1 \\subseteq S_2 \\subseteq V\\) 則 \\(span(S_1) \\subseteq span(S_2)\\) \\(span(S_1)\\) \\(\\cup\\) \\(span(S_2) \\subseteq span(S_1\\) \\(\\cup\\) \\(S_2)\\) \\(span(S_1)\\) \\(\\cap\\) \\(span(S_2) \\supseteq span(S_1\\) \\(\\cap\\) \\(S_2)\\) \\(S\\) 為 \\(W\\) 的子空間，則 \\(span(S) \\subseteq W\\) 2.1.7 和空間的生成集 若 \\(W_1 = span(S_1)\\), \\(W_2 = span(S_2)\\)，則\\(W_1 + W_2 = span(S_1\\) \\(\\cup\\) \\(S_2) = span(S_1) + span(S_2)\\) Note : 求和空間時，就取個別生成集聯集 2.2 Linear independent and Linear dependent (線性獨立與線性相依) 假設 \\(V_F\\) 為向量空間且有子集 \\(S\\)，給定有現向量 \\(v_1,\\cdots,v_k \\in S\\) 和純量 \\(c_1,\\cdots,c_k \\in F\\) 若 \\(c_1v_1 + \\cdots + c_kv_k = 0 \\iff c_i \\neq 0\\) for some \\(1\\leq i \\leq k\\)，則稱 \\(S\\) 為線性相依集 若 \\(c_1v_1 + \\cdots + c_kv_k = 0 \\iff c_i = 0\\) for all \\(1\\leq i \\leq k\\)，則稱 \\(S\\) 為線性獨立集 Note : 我們約定 \\(\\phi\\) 為線性獨立集 向量空間的子集不是線性相依就是線性獨立 \\(\\{0\\}\\) 是線性相依集 \\(\\implies\\) 所有含有 \\(\\{0\\}\\) 的子集向量必是線性相依集 若 \\(S\\) 為線性相依集，則表示 \\(S\\) 中的元素可以表示為其他元素線性組合 包含線性相依的子集必是線性相依，線性獨立集的子集必是線性獨立 \\(\\{v_1,\\cdots,v_n\\} \\in \\mathbb{R}^n\\) 是線性獨立集，則 \\(\\{Av_1,\\cdots,Av_n\\} \\in \\mathbb{R}^n\\) 是線性獨立集 \\(\\iff A\\) 可逆 2.2.1 線性獨立判別法 依照定義做驗證 以列向量的形式排成矩陣做列運算 \\(\\mbox{Wronskian}\\) 線性獨立判別法 (多用在函數的向量空間判斷) 設 \\(f_1, \\cdots, f_n \\in C^{(n-1)}[a,b]\\)，\\(f_1, \\cdots, f_n\\) 的 \\(\\mbox{Wromskian}\\) 定義為 \\[ W[f_1, \\cdots, f_n](x) = \\begin{vmatrix} f_1(x) &amp; f_2(x) &amp; \\cdots &amp; f_n(x)\\\\ f_1^{\\prime}(x) &amp; f_2^{\\prime}(x) &amp; \\cdots &amp; f_n^{\\prime}(x)\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ f_1^{(n-1)}(x) &amp; f_2^{(n-1)}(x) &amp; \\cdots &amp; f_n^{(n-1)}(x) \\end{vmatrix} \\] 如果存在 \\(x_0 \\in [a, b]\\) such that \\(W[f_1, \\cdots, f_n](x) \\neq 0\\) , 則 \\(f_1,\\cdots,f_n\\) 為線性獨立集 2.3 Basis and Space Property (基底與空間特性) 向量空間 \\(V_F\\) 的子集 \\(S\\) 如果滿足 \\(span(S) = V\\) 和 \\(S\\) 為獨立集，則稱 \\(S\\) 為 \\(V\\) 的一組基底 向量空間的維度被定義為基底的基數 (the dimension of vector space is the cardinality of a basis) Note : 我們約定 \\(\\{0\\}\\) 的基底是 \\(\\phi\\) 一個不為獨立集的生成集必可移除一些向量使其成為一組基底 一個不為生成集的獨立集必可添加一些向量使其成為一組基底 \\(S\\) 為 \\(V\\) 的基底 \\(\\iff\\) \\(S\\) 是 \\(V\\) 的最小生成集 \\(\\iff\\) \\(S\\) 是 \\(V\\) 的最大獨立集 \\(\\{v_1,\\cdots,v_n\\} \\in \\mathbb{R}^n\\) 是一組基底，則 \\(\\{Av_1,\\cdots,Av_n\\} \\in \\mathbb{R}^n\\) 是一組基底 \\(\\iff A\\) 可逆 2.3.1 四大子空間的基底 求 \\(A\\) 四大子空間基底，先做列運算 \\(A \\rightarrow R\\)，則我們有 nonzero row vectors of \\(R\\) form a basis of \\(\\mbox{RS}(A)\\) the pivot of \\(R\\) corresponse to column vectors of \\(A\\) form a basis of \\(\\mbox{CS}(A)\\) \\(\\mbox{ker}(A) = \\mbox{ker}(R)\\) , therefore the basis of \\(\\mbox{ker}(R)\\) form a basis of \\(\\mbox{ker}(A)\\) \\(L\\mbox{ker}(A) = \\mbox{RS}(A)^{\\bot}\\) , therefore the vectors which perpendecular to the vectors in \\(\\mbox{RS}(A)\\) form a basis of \\(L\\mbox{ker}(A)\\) if \\(A = PR\\) and \\(\\mbox{rank}(A) = r\\) , \\(P\\) is invertible, then exactly \\(r\\) column vectors from mostleft form a basis of \\(\\mbox{CS}(A)\\) Note : \\(A = PR\\) means that \\(A\\) is row equivalent to \\(R\\) and the procedure are represented by matrix \\(P\\) 2.3.1.1 Dimension of Sum Space (和空間維度) \\(\\mbox{dim}(V) &lt; \\infty\\)，且 \\(W_1\\), \\(W_2\\subseteq V_F\\) ，則 \\(\\mbox{dim}(W_1 + W_2) = \\mbox{dim}(W_1) + \\mbox{dim}(W_2) - \\mbox{dim}(W_1\\) \\(\\cap\\) \\(W_2)\\) Note : \\(\\mbox{dim}(W_1 + W_2) = \\mbox{dim}(W_1) + \\mbox{dim}(W_2) \\iff W_1\\) \\(\\cap\\) \\(W_2 = \\{0\\}\\) 和空間維度公式不適用於排容原理 2.4 Linear transformation (線性映射) 在佈於 \\(F\\) 上的向量空間 \\(V\\) , \\(W\\)，若函數 \\(T:V \\rightarrow W\\) 滿足 \\(\\forall\\) \\(u, v \\in V\\) , \\(c \\in F\\) 有 \\(T(u+cv) = T(u) + cT(v)\\)，則稱 \\(T\\) 為從 \\(V\\) 到 \\(W\\) 的線性映射，寫作 \\(T \\in L(V, W)\\) 令 \\(X,Y\\) 為 \\(V,W\\) 的子集 \\(T(X) = \\{T(v)\\mid v \\in X\\}\\) 稱為 \\(X\\) 之於 \\(T\\) 的像 (image) \\(T^{-1}(Y) = \\{v \\in V\\mid T(v) \\in Y\\}\\) 稱為 \\(Y\\) 之於 \\(T\\) 的反像 (preimage) Note : \\(T(0) = 0\\) 且 \\(T^{-1}\\) 並非 \\(T\\) 的反函數 微分運算與積分運算為線性映射且有不可逆的矩陣表示法 若 \\(T,U \\in L(V,W)\\)，則 \\(T = U \\iff T(b_i) = U(b_i)\\) 其中 \\(b_i\\) , \\(1 \\leq i \\leq n\\) 形成 \\(V\\) 的一組基底 對於同一個有序基底，線性變換存在唯一的矩陣表示法 2.5 Rank of a Matrix (矩陣的秩) 對於任意矩陣 \\(A\\) , \\(\\mbox{dim}(\\mbox{RS}(A))\\) 稱為 row rank of \\(A\\) 對於任意矩陣 \\(A\\) , \\(\\mbox{dim}(\\mbox{CS}(A))\\) 稱為 column rank of \\(A\\) The definition of the rank of a matrix \\(A\\) is the dimension of Vector space spanned by the columns of \\(A\\), or the dimension of Vector space spanned by the row of \\(A\\), equivalently, \\(\\mbox{rank} = \\mbox{dim}(\\mbox{RS}(A)) = \\mbox{dim}(\\mbox{CS}(A))\\) (詳見如何找四大空間基底) Note : 矩陣的秩可以看做可控參數的數目，而核空間的維度可以看做自由參數的數目 (參考自由度) \\(A \\in F^{n \\times n}\\) 可逆 \\(\\iff\\) \\(A\\) 是滿秩的 (full rank and \\(\\mbox{rank}(A) = n\\)) 2.5.1 Sylvester Theorem (秩 零度定理) 假設線性變換 \\(T \\in L(V,W)\\) 有矩陣表示法 \\(A \\in F^{m \\times n}\\) 且 \\(\\mbox{dim}(V) &lt; \\infty\\)，則以下數學式常用 \\(n = nullity(A) + \\mbox{rank}(A)\\) \\(m = nullity(A^T) + \\mbox{rank}(A^T) = nullity(A^T) + \\mbox{rank}(A)\\) \\(\\mbox{dim}(V) = nullity(T) + \\mbox{rank}(T)\\) \\(V = \\mbox{ker}(T) + \\mbox{Im}(T) \\iff \\mbox{ker}(T)\\) \\(\\cap\\) \\(\\mbox{Im}(T) = \\{0\\}\\) 2.5.2 Property of the Rank (秩的特性) \\(\\mbox{rank}(A) = \\mbox{rank}(A^T)\\) \\(\\mbox{rank}(A) \\leq \\min\\{m,n\\}\\) If \\(A\\) is row / column equivalent to \\(R\\) then \\(\\mbox{rank}(A) = \\mbox{rank}(B)\\) \\(\\mbox{rank}(AB) \\leq \\min \\{\\mbox{rank}(A), \\mbox{rank}(B)\\}\\) 若方陣 \\(P\\) 可逆，則 \\(\\mbox{rank}(PA) = \\mbox{rank}(A)\\) , \\(\\mbox{rank}(AP) = \\mbox{rank}(A)\\) \\(\\mbox{rank}(A+ B) \\leq \\mbox{rank}(A) + \\mbox{rank}(B)\\) \\(Ax = b\\) is consistent \\(\\iff\\) \\(\\mbox{rank}([A \\mid b]) = \\mbox{rank}(A)\\) \\(\\forall\\) \\(b \\in \\mathbb{R}^{m \\times 1}\\) , \\(Ax = b\\) is consistent \\(\\iff \\mbox{rank}(A) = m\\) \\(m=n \\implies\\) exactly one solution \\(m &lt; n \\implies\\) infinty solutions Clearly explaintion : 矩陣的轉置會把 Column space 和 Row space 對調，並不影響他們的基底，故秩相同 Column space 和 Row space 的維度相同，故矩陣秩最大只能到\\(\\min \\{\\mbox{dimension of Column / Row space}\\}\\) 行列運算並不改變 Column space 和 Row sapce，故秩相同 \\(AB\\) 可以看做 \\(A\\) column vectors 的線性組合或 \\(B\\) row vectors 的線性組合，故 \\(AB\\) 的秩較小 \\(PA\\) 中的 \\(P\\) 可以看做基本列運算，故 \\(A\\) is row equivalent to \\(PA\\)，故秩相同，同理可得後者 矩陣的秩可以看做可控參數的數目，故分別可控的數量必比和空間要多 (\\(A\\) 中可能存在某些向量與 \\(B\\) 中的向量線性相依) 增廣矩陣秩不變表示 \\(b\\) 為 \\(A\\) 中 column vectors 的線性組合，也就是說 \\(b \\in \\mbox{CS}(A)\\) 由秩零度定理知，\\(nullity(A) &gt; 0\\)，自由參數大於 \\(1\\) ，故無限多組解 2.6 Direct Sum and Projection (直和與投影) 對於向量空間 \\(V_F\\) 與子空間 \\(W_1, \\cdots, W_k\\)，若 \\(W_i\\) \\(\\cap\\) \\(\\displaystyle\\sum_{i\\neq j}W_j = \\{0\\}\\) where \\(1\\leq i \\leq k\\) 則稱 \\(W_1, \\cdots, W_k\\) 為獨自子空間，且若有 \\(V = \\displaystyle \\sum_{i=1}^kW_i\\)，則稱 \\(W_1, \\cdots, W_k\\) 為 \\(V\\) 的一個直和，記做 \\(V = W_1 \\oplus W_2 \\cdots \\oplus W_k\\) Note : 對於任意 \\(v \\in V\\) 存在唯一方式分解成 \\(\\sum w_i\\) for \\(1\\leq i\\leq k\\) 其中 \\(w_i \\in W_i\\) 任何一個方陣必可分解成一個對稱矩陣加上一個反對稱矩陣 任何一個實函數必可分解成一個奇函數加上一個偶函數 2.6.1 Projection Matrix (投影矩陣) 若 \\(T \\in L(V,V)\\) 滿足 \\(T^2 = T\\) 則稱 \\(T\\) 為 \\(V\\) 上的投影/冪等算子 (projection / idempotent operator) 若 \\(A \\in F^{m\\times n}\\) 滿足 \\(A^2 = A\\) 則稱 \\(A\\) 為投影/冪等矩陣 (projection / idempotent matrix) Note : 投影矩陣投影兩次等於投影一次 投影矩陣把被投影的向量投影到該矩陣的 Column space 上 \\(I_n - \\frac{1}{n}J_n\\) 為投影矩陣 若 \\(A,B\\) 皆為對稱矩陣，且滿足 \\(AB\\) 為投影矩陣，則 \\(BA\\) 為投影矩陣 若 \\(A\\) 為投影矩陣，則 \\(I - A\\) 也是投影矩陣 若方陣 \\(A\\) 為投影矩陣，則 \\(A = I\\) 或 \\(A\\) 不可逆 Clearly explaintion : 把向量 \\(x\\) 投影到 \\(A\\) 的 Coumn space 上後，\\(x \\in \\mbox{CS}(A)\\)，在投影一次也是一樣的結果 \\(Ax\\) 代表 \\(A\\) column vectors 的線性組合，自然投影到 \\(\\mbox{CS}(A)\\) 上，且投影兩次結果不變 \\((I_n - \\frac{1}{n}J_n)^2 = I_n - \\frac{2}{n}J_n + \\frac{1}{n^2}J_n^2 = I_n - \\frac{1}{n}J_n\\) \\((BA)^2 = ((A^TB^T)^T)^2 = ((AB)^T)^2 = ((AB)^2)^T = (AB)^T = BA\\) 投影矩陣把向量投影到不同的獨立子空間後，根據直和的定義，他在各個獨立子空間有唯一分解表達式，故把表達式加總後會回到原來的向量，也就是說 \\(I_n = (I_n - A) \\oplus A\\) 為投影矩陣 (詳見譜分解定理) 如果投影矩陣把原空間投影到相同的空間，則投影矩陣為單位矩陣 如果投影矩陣把原空間投影到更低的維度時，他必然捨棄一些維度，故 \\(A\\) 不可逆 2.6.2 Orthogonal Projection (正交投影) 若 \\(V = X\\oplus Y\\)， 根據直和定義 \\(\\forall \\ z \\in V\\) \\(z =x + y\\) 其中 \\(x \\in X\\) , \\(y \\in Y\\)，且線性算子 \\(T(z) = x\\)，則稱線性算子 \\(T\\) 為沿著 \\(Y\\) 在 \\(X\\) 上的投影，更進一步，若 \\(X\\) 正交於 \\(Y\\)，則稱為正交投影 Example : 在 \\(\\mathbb{R}^3\\) 中，\\(\\mathbb{R}^3 = P \\oplus L\\)，其中 \\(P\\) 為一平面，\\(L\\) 為一條直線，且 \\(L\\) 不在 \\(P\\) 上，則任何向量 \\(z \\in \\mathbb{R}^3\\) 皆可表示為 \\(P\\) 上的點和 \\(P\\) 外的點的和，而 \\(P\\) 上的點為 \\(P\\) 外的點沿著 \\(L\\) 在 \\(P\\) 上的投影 2.7 Change Basis and Coordinates Matrix (基底變換與基底變換矩陣) 假設 \\(\\beta = \\{b_1 ,\\cdots ,b_n\\}\\) 為向量空間 \\(V\\) 的一組有序基底，則任何一個向量 \\(v \\in V\\) 可以用 \\(\\beta\\) 中的基底表示，有 \\(v = c_1b_1 + \\cdots + c_nb_n\\)，且是唯一表示，記做 \\(\\begin{bmatrix}v\\end{bmatrix}_{\\beta}\\) 假設有另外一組有序基底 \\(\\alpha = \\{a_1,\\cdots,a_n\\}\\)，令矩陣 \\(A, B\\) 的 Column vector 分別由 \\(\\alpha, \\beta\\) 表示， 定義 \\(\\alpha\\) 到 \\(\\beta\\) 的基底變換矩陣為 \\(P = [\\ [a_1]_{\\beta}, \\cdots, [a_n]_{\\beta}\\ ]\\) 其 Colmn vector 為 \\(A\\) 的 Column vector 在 \\(\\beta\\) 基底下的表示法係數，且有以下關係 : \\(\\forall\\ v \\in V\\) , \\(\\begin{bmatrix}v\\end{bmatrix} _{\\beta} = P\\begin{bmatrix}v\\end{bmatrix}_{\\alpha}\\) \\(BP = A\\) or equivalently \\(P = B^{-1}A\\) Clearly explaintion : \\(\\begin{bmatrix}v\\end{bmatrix}_{\\alpha}\\) 為在 \\(\\alpha\\) 基底下的表示係數，而矩陣 \\(P\\) 為 \\(\\alpha\\) 基底在 \\(\\beta\\) 基底下的表示係數 考慮 \\(BPv = Av\\)， \\(Av\\) 為 \\(\\alpha\\) 中基底的線性組合，即用 \\(\\alpha\\) 基底來看向量 \\(v\\) \\(BPv\\) 為 \\(\\beta\\) 中基底的線性組合，即用 \\(\\beta\\) 基底來看向量 \\(v\\)，故兩者相同 2.8 線性變換矩陣表示法 求性線變換 \\(T \\in L(V,V)\\) 相對於 \\(\\alpha\\) 的矩陣 \\(A\\) 表示法，先看基於哪一組基底 \\((\\alpha)\\) (一般為標準正交基) ，將基底帶入線性算子中即為 \\(A\\) 的 Column vector，再用 \\(\\alpha\\)，記做 \\(\\begin{bmatrix}T\\end{bmatrix}_{\\alpha}\\) \\[ A = \\begin{bmatrix} \\mid &amp; &amp; \\mid \\\\ [T(a_1)]_{\\alpha} &amp; \\cdots &amp; [T(a_n)]_{\\alpha} \\\\ \\mid &amp; &amp; \\mid \\end{bmatrix} \\] 求性線變換 \\(T \\in L(V,W)\\) 相對於 \\(\\alpha\\) 到 \\(\\beta\\) 的矩陣 \\(A\\) 表示法，先看基於哪一組基底 \\((\\alpha)\\) (一般為標準正交基) ，將基底帶入線性算子中再用 \\(\\beta\\) 的基底表示，即為 \\(A\\) 的 Column vector，記做 \\(\\begin{bmatrix}T\\end{bmatrix}_{\\alpha}^{\\beta}\\) \\[ A = \\begin{bmatrix} \\mid &amp; &amp; \\mid \\\\ \\begin{bmatrix}T(a_1)\\end{bmatrix}_{\\beta} &amp; \\cdots &amp; \\begin{bmatrix}T(a_n)\\end{bmatrix}_{\\beta} \\\\ \\mid &amp; &amp; \\mid \\end{bmatrix} \\] Note : (這裡假設 \\(T\\in L(V,W)\\) 且 \\(\\alpha, \\gamma\\) 為 \\(V\\) 的一組基底 \\(\\beta, \\gamma&#39;\\) 為 \\(W\\) 的一組基底) \\(\\forall\\ v \\in V\\) , \\(\\begin{bmatrix}T(v)\\end{bmatrix} _{\\beta} = \\begin{bmatrix}T\\end{bmatrix}_{\\alpha}^{\\beta}\\begin{bmatrix}v\\end{bmatrix}_{\\alpha}\\) \\(\\begin{bmatrix}T\\end{bmatrix}_{\\alpha}^{\\beta} = \\begin{bmatrix}I\\end{bmatrix}_{\\gamma&#39;}^{\\beta}\\begin{bmatrix}T\\end{bmatrix}_{\\gamma}^{\\gamma&#39;}\\begin{bmatrix}I\\end{bmatrix}_{\\alpha}^{\\gamma}\\) 2.8.1 Similarity of the Square Matrix (方陣的相似) 考慮方陣 \\(A, B\\) 若存在 可逆矩陣 \\(P\\) 滿足 \\(B = P^{-1}AP\\) 則稱 \\(A\\) 與 \\(B\\) 相似 記做 \\(A\\) \\(\\sim\\) \\(B\\) Note : (假設 \\(A \\sim B\\) ) 方陣的相似性為等價關係 \\(I\\) 只與 \\(I\\) 相似， \\(O\\) 只與 \\(O\\) 相似 同一個線性變換在不同基底之下的矩陣表示法相似 \\(A^T \\sim B^T\\) 和 \\(A^k \\sim B^k\\) \\(A + cI \\sim B + cI\\) \\(f(A) \\sim f(B)\\) 其中 \\(f\\) 為多項式函數 若 \\(A, B\\) 可逆，則 \\(A^{-1} \\sim B^{-1}\\) \\(\\mbox{tr}(A) = \\mbox{tr}(B)\\) \\(\\mbox{det}(A) = \\mbox{det}(B)\\) \\(\\mbox{rank}(A) = \\mbox{rank}(B)\\) 且 \\(nullity(A) = nullity(B)\\) 反過來說，以上一項不成立，則 \\(A \\not\\sim B\\) 2.9 Eigenvalue and Eigenvetor (特徵值與特徵向量) 對於矩陣 \\(A \\in F^{n \\times n}\\)，若存在 \\(\\lambda \\in \\mathbb{R}\\) 和非零向量 \\(v \\in F^{n\\times 1}\\)，使得 \\(Av = \\lambda v\\)，則稱 \\(\\lambda\\) 為矩陣 \\(A\\) 的特徵值，\\(v\\) 為矩陣 \\(A\\) 的特徵向量 Note : \\(\\lambda\\) 為矩陣 \\(A\\) 的特徵值 \\(\\iff \\mbox{det}(A - \\lambda I) = 0\\) \\(\\mbox{ker}(A)\\) 中的非零向量均為特徵值 \\(0\\) 對應的特徵向量 若 \\(v_1, v_2\\) 為相對於 \\(\\lambda\\) 的特徵向量，則 \\(v_1 +v_2\\) \\((\\neq 0)\\) 亦為相對於 \\(\\lambda\\) 的特徵向量 並非每個矩陣都有特徵值 若 \\(A \\sim B\\)，則 \\(A\\) 和 \\(B\\) 有相同特徵值與相對應的特徵向量 \\(v, P^{-1}v\\) \\(AB\\) 和 \\(BA\\) 有相同特徵值 \\(A\\) 和 \\(A^T\\) 有相同特徵值 若 \\(A\\) 的各行(列)元素和均為 \\(c\\)，則必有特徵值 \\(c\\) \\(A\\) 可逆 \\(\\iff\\) \\(0\\) 不為 \\(A\\) 的一個特徵值 如果 \\(\\lambda\\) 是 \\(A\\) 相對於 \\(v\\) 的特徵值，則 \\(\\frac{1}{\\lambda}\\) 為 \\(A^{-1}\\) 相對於 \\(v\\) 的特徵值 \\(\\lambda^k\\) 為 \\(A^k\\) 相對於 \\(v\\) 的特徵值 \\(f(\\lambda)\\) 為 \\(f(A)\\) 相對於 \\(v\\) 的特徵值，其中 \\(f\\) 為多項式函數 2.9.1 characteristic polynomial (特徵多項式) 假設 \\(A \\in F^{n\\times n}\\)，則特徵多項式被定義為 \\(char_A(\\lambda) = \\mbox{det}(A - \\lambda I)\\)，即解特徵值的方程式 Note : 若 \\(A \\sim B\\)，則 \\(char_A(\\lambda) = char_B(\\lambda)\\) \\(A, B \\in F^{n \\times n}\\)，則 \\(char_{AB}(\\lambda) = char_{BA}(\\lambda)\\) \\(char_A(\\lambda) = (-\\lambda)^n + tr_1(A)(-\\lambda)^{n-1} + \\cdots + tr_{n-1}(A)(-\\lambda) + tr_n(A)\\)，其中 \\(tr_i(A)\\) 為 \\(A\\) 中所有恰含 \\(i\\) 的對角項的 \\(i\\) 階子行列式的和 若 \\(A\\) 有特徵值 \\(\\lambda_1,\\cdots, \\lambda_k\\)，則 \\(\\lambda_1 + \\cdots + \\lambda_k = \\mbox{tr}(A)\\)，\\(\\lambda_1\\lambda_2\\cdots \\lambda_k = \\mbox{det}(A)\\) 2.9.2 Eigenspace and multiplicity (特徵空間和重數) 假設 \\(A \\in F^{n\\times n}\\)，\\(v \\in F^{n\\times 1}\\) 為 \\(A\\) 的特徵向量，定義 \\(E_{\\lambda_i} = \\{v \\mid Av = \\lambda_i v，\\lambda_i \\in \\mathbb{R}\\} = \\mbox{ker}(A - \\lambda I)\\) 為 \\(\\lambda_i\\) 的特徵空間 代數重數為特徵值 \\(\\lambda_i\\) 的重根個數，記做 \\(am(\\lambda_i)\\) 幾何重數為特徵空間 \\(E_{\\lambda_i}\\) 的維度，記做 \\(gm(\\lambda_i)\\) Note : \\(\\lambda\\) 對應的特徵向量為 \\(E_{\\lambda_i}\\) 中的非零向量 \\(\\lambda_1,\\cdots,\\lambda_k\\) 為相異特徵值，則 \\(E_{\\lambda_1},\\cdots,E_{\\lambda_k}\\) 為獨立子空間 \\(gm(\\lambda) = \\mbox{dim}(E_{\\lambda}) = nullity(T - \\lambda I) = n - \\mbox{rank}(T - \\lambda I)\\) \\(gm(0) = \\mbox{dim}(E_0) = nullity(T) = n - \\mbox{rank}(A)\\) \\(1 \\leq gm(\\lambda) \\leq am(\\lambda) \\leq n\\) 若 \\(\\mbox{rank}(A) = k\\)，則 \\(A\\) 的相異特徵值最多 \\(k\\) 個 2.10 Diagonalizable (可對角化) 假設 \\(A \\in F^{n\\times n}\\)，若存在可逆矩陣 \\(P\\) 使得 \\(P^{-1}AP = D\\)，其中 \\(D\\) 為對角矩陣，則稱 \\(A\\) 可對角化，即 \\(A \\sim D\\) Clearly explantion on diagonalizable: 矩陣對角化的精神在於用特徵向量當作基底，來表達任何一個在 \\(V\\) 中的向量，換句話說，如果特徵向量為 \\(V\\) 的生成集，則 \\(v \\in V\\) 必可表示為特徵向量之和，過程如下 : Input : a square matrix \\(A\\) Compute the characteristic polynomial \\(char_A(\\lambda) = \\mbox{det}(A - \\lambda I)\\) Factor \\(char_A(\\lambda) = (\\lambda - \\lambda_1)^{m1}(\\lambda - \\lambda_2)^{m2}\\cdots(\\lambda - \\lambda_k)^{mk}\\) For each \\(\\lambda_i\\) , compute a basis of \\(E_{\\lambda_i} = \\mbox{ker}(A - \\lambda_i I)\\) and let \\(d_i = \\mbox{dim}(E_{\\lambda_i})\\) If \\(d_i = m_i\\) for \\(i = 1,\\cdots,k\\) then go to \\(5.\\) If \\(d_i \\neq m_i\\) for some \\(i\\) , then not diagonalizable \\[D = \\begin{bmatrix} \\lambda_1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; \\lambda_k\\end{bmatrix} \\mbox{and}\\ P = \\begin{bmatrix}\\mid &amp; &amp; \\mid \\\\ v_1 &amp; \\cdots &amp; v_k \\\\ \\mid &amp; &amp; \\mid\\end{bmatrix} \\ \\mbox{where $v_i$ are the eigenbasis of $E_{\\lambda_i}$}\\] \\(A = PDP^{-1}\\) or \\(D = P^{-1}AP\\) Note : \\(A \\in F^{n\\times n}\\) 可對角化 \\(\\iff A\\) 有 \\(n\\) 個線性獨立的特徵向量 \\(A \\in F^{n\\times n}\\) 有 \\(n\\) 個相異的特徵值，則 \\(A\\) 有 \\(n\\) 個相互獨立的特徵向量 若 \\(A\\) 可對角化，則 \\(A^T, A^{-1}, A^k, f(A)\\) 皆可對角化，其中 \\(f\\) 為多項式函數 \\(A\\) 可否對角化與可逆無關 ( \\(O\\) 矩陣可對角化) 實對稱矩陣皆可對角化 \\(A\\) 可對角化 \\(\\iff\\) \\(am(\\lambda_i) = gm(\\lambda_i)\\) for \\(i = 1,\\cdots,k\\) \\(A\\) 的特徵值皆為 \\(\\pm 1 \\implies\\) \\(A^2 = I\\) 若 \\(A\\) 為投影矩陣，則 \\(A\\) 必可對角化，且 \\(A\\) 的特徵值為 \\(0\\) 或 \\(1\\) 若 \\(A\\) 為投影矩陣，則 \\(E_{0} = \\mbox{ker}(A)\\) 和 \\(E_{1} = \\mbox{CS}(A)\\) 2.11 Eigen Value Decomposition (特徵值分解) 特徵值分解為對角化的特例，若 \\(A \\in F^{n\\times n}\\) 為對稱矩陣，則 \\(A\\) 的特徵向量會正交，我們令各個特徵基底的長度為 \\(1\\)，則特徵基底會形成一組正交基，則我們可以分解矩陣 \\(A\\) 為向量投影到不同特徵空間 \\[ A = \\sum_{i=1}^k\\lambda_i V_i\\ V_i^T = \\sum_{i = 1}^k\\lambda_i P_i \\] \\(\\lambda_i\\) 為 \\(A\\) 的特徵值，\\(v_i\\) 為對應於 \\(\\lambda_i\\) 的特徵值 \\(V_i\\) 的 Column vectors 為 \\(E_{\\lambda_i}\\) 中的基底，(各個 \\(V_i\\) 大小取決於\\(E_{\\lambda_i}\\) 的維度) \\(P_i = V_i\\ V_i^T\\)，有 \\(P_i \\ P_j \\neq 0\\) for \\(i \\neq j\\) 和 \\(\\sum P_i = I\\) , \\(P_i^2 = P_i\\) Clearly explantion : (為了便於解釋，假設所有特徵值相異) 考慮 \\(V V^Tu\\)，向量 \\(u\\) 投影到 \\(v\\) 上的公式是 \\(\\frac{&lt;v,u&gt;}{||v||^2}v\\)，而 \\(VV^Tu\\) 正是把 \\(u\\) 投影到特徵向量上表示出來 : \\[ VV^Tu = \\begin{bmatrix} \\mid &amp; &amp; \\mid \\\\ v_1 &amp; \\cdots &amp; v_n \\\\ \\mid &amp; &amp; \\mid \\end{bmatrix}\\begin{bmatrix} \\mid &amp; &amp; \\mid \\\\ v_1 &amp; \\cdots &amp; v_n \\\\ \\mid &amp; &amp; \\mid \\end{bmatrix}^T\\begin{bmatrix} u_0\\\\ \\vdots\\\\ u_n \\end{bmatrix} = \\begin{bmatrix} \\mid &amp; &amp; \\mid \\\\ v_1 &amp; \\cdots &amp; v_k \\\\ \\mid &amp; &amp; \\mid \\end{bmatrix}\\begin{bmatrix} &lt;v_1,u&gt;\\\\ \\vdots\\\\ &lt;v_n,u&gt; \\end{bmatrix} \\] \\(\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = \\ &lt;v_1,u&gt;v_1 + &lt;v_2,u&gt;v_2 + \\cdots + &lt;v_n,u&gt;v_n\\) \\(\\mbox{Which represent as a projection onto Eigenspace than scale by the value of inner product.}\\) \\(\\mbox{An alternative view is direct sum, that is, seperate a vector into independent Eigen subspace.}\\) \\(\\mbox{Note that}\\) \\(P_i\\) \\(\\mbox{stand for the projection matrix, and Vector space}\\) \\(V = E_{\\lambda_1}\\oplus\\cdots\\oplus E_{\\lambda_k}\\) \\(\\mbox{therefore, every Eigenspace is independent subspace, which implies}\\) \\(P_i \\ P_j = 0 \\ \\mbox{for} \\ i \\neq j.\\) \\(\\mbox{Moreover, the direct sum indicates that}\\) \\(P_i\\) \\(\\mbox{contains a part of representation of}\\) \\(v\\) \\(\\mbox{and it is}\\) \\(\\mbox{unique, hence, we paste every component together will return to}\\) \\(v\\) \\(\\mbox{itself, which implies that}\\) \\(\\sum P_i = I.\\) 2.11.1 Simulltaneously diagonalizable (同步對角化) 設 \\(A, B \\in F^{n\\times n}\\) 若存在可逆矩陣 \\(P \\in F^{n\\times n}\\) 使得 \\(P^{-1}AP = D\\)，\\(P^{-1}BP = \\Lambda\\) 皆為對角矩陣，則稱 \\(A, B\\) 可同步對角化 Note : \\(A, B\\) 可同步對角化 \\(\\iff AB = BA\\) 2.11.2 Diagonalization and function limit (對角化與函數極限) 定義 \\(e^A = \\displaystyle \\sum_{n=0}^{\\infty}\\frac{A^n}{n!}\\)，\\(\\sin A, \\cos A\\) 也是同樣的定義方法 若 \\(B\\) 滿足 \\(B^2 = A\\)，則稱 \\(B = A^{1/2}\\) 假設 \\(A_1, \\cdots, A_n\\) 為方陣形成的序列，若 \\(\\displaystyle\\lim_{k \\to \\infty}(A_k)_{ij} = A_{ij}\\)，則稱序列極限存在且收斂到 \\(A\\)， 記做 \\(\\displaystyle \\lim_{k \\to \\infty}A_k = A\\) Note : 若 \\(A \\sim B\\)，則 \\(e^A \\sim e^B\\) 若 \\(\\lambda\\) 為 \\(A\\) 相對於 \\(v\\) 的特徵值，則 \\(e^{\\lambda}\\) 為 \\(e^A\\) 相對於 \\(v\\) 的特徵值 \\(\\mbox{det}(e^A) = e^{tr(A)}\\) 若 \\(A\\) 可對角化成 \\(D\\)，則 \\(e^A = P\\ e^DP^{-1}\\) 為可逆矩陣 若 \\(A\\) 為實對稱矩陣，則 \\(e^A\\) 為對稱且正定矩陣 若 \\(B = PAP^{-1}\\)，則 \\(\\displaystyle \\lim_{k \\to \\infty}B^k = PDP^{-1}\\)，其中 \\(\\displaystyle\\lim_{k \\to \\infty}A^k = D\\) 可對角矩陣的特徵值若滿足 \\(\\mid\\ \\lambda\\mid \\leq 1\\)，則此對角矩陣會收斂 Clearly explaintion : 假設存在可逆矩陣 \\(P\\) 使得 \\(B = PAP^{-1}\\)，則 \\(e^B = \\sum\\frac{B^n}{n!} = \\sum\\frac{(PAP^{-1})^n}{n!} = \\sum\\frac{PA^nP^{-1}}{n!} = Pe^AP^{-1}\\)，故 \\(e^A \\sim e^B\\) 假設 \\(Av = \\lambda v\\)，則 \\(e^Av = \\sum\\frac{A^nv}{n!} = \\sum \\frac{\\lambda^nv}{n!} = e^{\\lambda}v\\)，故相對於 \\(v\\) 特徵值為 \\(e^{\\lambda}\\) 矩陣行列式為特徵值相乘，故得此結論 \\(e^D\\) 為正定矩陣，故特徵值皆為正數 \\(\\implies \\mbox{det}(e^D) \\neq 0\\) 若 \\(A^T = A\\)，則 \\((e^A)^T = \\sum \\frac{(A^T)^n}{n!} = \\sum \\frac{A^n}{n!} = e^A\\)，且有特徵值 \\(e^{\\lambda_i} \\geq 0\\)，故 \\(e^A\\) 為對稱正定矩陣 2.11.3 Simulltaneously diagonalizable (同步對角化) 設 \\(A, B \\in F^{n\\times n}\\) 若存在可逆矩陣 \\(P \\in F^{n\\times n}\\) 使得 \\(P^{-1}AP = D\\)，\\(P^{-1}BP = \\Lambda\\) 皆為對角矩陣，則稱 \\(A, B\\) 可同步對角化 Note : \\(A, B\\) 可同步對角化 \\(\\iff AB = BA\\) 2.11.4 Diagonalization and function limit (對角化與函數極限) 定義 \\(e^A = \\displaystyle \\sum_{n=0}^{\\infty}\\frac{A^n}{n!}\\)，\\(\\sin A, \\cos A\\) 也是同樣的定義方法 若 \\(B\\) 滿足 \\(B^2 = A\\)，則稱 \\(B = A^{1/2}\\) 假設 \\(A_1, \\cdots, A_n\\) 為方陣形成的序列，若 \\(\\displaystyle\\lim_{k \\to \\infty}(A_k)_{ij} = A_{ij}\\)，則稱序列極限存在且收斂到 \\(A\\)， 記做 \\(\\displaystyle \\lim_{k \\to \\infty}A_k = A\\) Note : 若 \\(A \\sim B\\)，則 \\(e^A \\sim e^B\\) 若 \\(\\lambda\\) 為 \\(A\\) 相對於 \\(v\\) 的特徵值，則 \\(e^{\\lambda}\\) 為 \\(e^A\\) 相對於 \\(v\\) 的特徵值 \\(\\mbox{det}(e^A) = e^{tr(A)}\\) 若 \\(A\\) 可對角化成 \\(D\\)，則 \\(e^A = P\\ e^DP^{-1}\\) 為可逆矩陣 若 \\(A\\) 為實對稱矩陣，則 \\(e^A\\) 為對稱且正定矩陣 若 \\(B = PAP^{-1}\\)，則 \\(\\displaystyle \\lim_{k \\to \\infty}B^k = PDP^{-1}\\)，其中 \\(\\displaystyle\\lim_{k \\to \\infty}A^k = D\\) 可對角矩陣的特徵值若滿足 \\(\\mid\\ \\lambda\\mid \\leq 1\\)，則此對角矩陣會收斂 Clearly explaintion : 假設存在可逆矩陣 \\(P\\) 使得 \\(B = PAP^{-1}\\)，則 \\(e^B = \\sum\\frac{B^n}{n!} = \\sum\\frac{(PAP^{-1})^n}{n!} = \\sum\\frac{PA^nP^{-1}}{n!} = Pe^AP^{-1}\\)，故 \\(e^A \\sim e^B\\) 假設 \\(Av = \\lambda v\\)，則 \\(e^Av = \\sum\\frac{A^nv}{n!} = \\sum \\frac{\\lambda^nv}{n!} = e^{\\lambda}v\\)，故相對於 \\(v\\) 特徵值為 \\(e^{\\lambda}\\) 矩陣行列式為特徵值相乘，故得此結論 \\(e^D\\) 為正定矩陣，故特徵值皆為正數 \\(\\implies \\mbox{det}(e^D) \\neq 0\\) 若 \\(A^T = A\\)，則 \\((e^A)^T = \\sum \\frac{(A^T)^n}{n!} = \\sum \\frac{A^n}{n!} = e^A\\)，且有特徵值 \\(e^{\\lambda_i} \\geq 0\\)，故 \\(e^A\\) 為對稱正定矩陣 "],["基礎微積分.html", "Chapter 3 基礎微積分 3.1 數列的極限 3.2 級數 3.3 純量場 3.4 向量場 3.5 線積分 3.6 格林定理", " Chapter 3 基礎微積分 3.1 數列的極限 數列收斂的定義，我們說一個數列\\({a_n}\\)收斂到\\(L\\)的定義如下: 對所有\\(\\epsilon&gt;0\\)，我們能找到一個數\\(M&gt;0\\)，使得\\(n&gt;M\\)時，有\\(|a_n - L|&lt;\\epsilon\\) 當超過數列某一項(M)之後，數列的值與L小於任何正數 如果不存在這麼一個\\(L\\)的話，我們說這組數列發散 如果收斂，數學上我們寫\\(\\lim_{n \\to \\infty}a_n = L\\) 如果兩組數列收斂，則他們滿足純量的四則運算 3.1.1 夾擠定理 如果數列\\({x_n}\\)和\\({y_n}\\)收斂到\\(L\\)，且數列\\(w_n\\)每一項都滿足\\(x_n \\leq w_n \\leq y_n\\)，則我們有\\(w_n\\)也收斂到\\(L\\) Example : Find \\(\\lim_{n \\to \\infty}2^{-n}(cos(n^3-n^2+n-13))\\) 3.1.2 比較定理 假設數列\\({x_n}\\)和\\({y_n}\\)都是收斂數列，如果在某一項\\((M)\\)之後都有\\({x_n} \\leq {y_n}\\)，則我們有\\(\\lim_{n\\to\\infty}x_n \\leq \\lim_{n\\to\\infty}y_n\\) 3.2 級數 簡單來說，就是把數列裡面的值加起來!! 部分和(partial sum)，就是把數列的一部分加起來，數學上我們寫\\(s_n := \\sum_{k=1}^na_k\\) 我們說一個級數收斂當且僅當\\(|s_n - s|&lt;\\epsilon\\)對所有\\(n&gt;N\\) 不收斂則發散，嚴謹地說，部分和不收斂或\\(\\sum_{k=1}^na_k = \\infty\\)及發散 比較好記的方法  級數收斂就是數列加總之後會到一個固定的值 \\(\\sum_{k=1}^{\\infty} (-1)^k\\) 是不收斂的，部分和不收斂，或是他在-1,1跳動，沒有跑到一個特定的值!! 我們稱一個級數絕對收斂當且僅當\\(\\sum_{k=1}^{\\infty}|a_k|&lt;\\infty\\) 我們稱一個級數條件收斂僅且僅當級數收斂但不是絕對收斂 3.2.1 數列收斂審歛法 在一般情況下，課本教一堆公式，但是有用的只有以下三個 Root Test 假設\\(a_k\\)是一個實數數列，定義\\(r = \\displaystyle\\limsup_{k \\to \\infty}|a_k|^{1/k}\\) 如果 \\(r&lt;1\\)，則\\(\\sum_{k=1}^{\\infty}a_k\\)絕對收斂 如果 \\(r&gt;1\\)，則\\(\\sum_{k=1}^{\\infty}a_k\\)發散 Ratio Test 假設\\(a_k\\)是一個實數數列，且對於\\(k \\to \\infty\\)，\\(a_k \\neq 0\\)，定義\\(r = \\displaystyle\\frac{\\lim_{k \\to \\infty}a_{k+1}}{\\lim_{k \\to \\infty}a_{k}}\\) 如果 \\(r&lt;1\\)，則\\(\\sum_{k=1}^{\\infty}a_k\\)絕對收斂 如果 \\(r&gt;1\\)，則\\(\\sum_{k=1}^{\\infty}a_k\\)發散 :notebook_with_decorative_cover: 如果\\(r=1\\)，以上兩種方法都無效，注意到他們的判斷內容完全一致 Compare Test 假設對於很大的\\(k\\)，我們有\\(0 \\leq a_k \\leq b_k\\)，則 如果\\(\\sum_{k=1}^{\\infty}b_k &lt; \\infty\\)，則\\(\\sum_{k=1}^{\\infty}a_k &lt; \\infty\\) 如果\\(\\sum_{k=1}^{\\infty}a_k = \\infty\\)，則\\(\\sum_{k=1}^{\\infty}b_k = \\infty\\) 解題時先看能不能用Ratio Test，在看Root Test!! 3.2.2 特殊級數 p級數 如果\\(k&gt;1\\)，則\\(\\sum_{k=1}^{\\infty}\\frac{1}{n^k}\\)收斂 望遠鏡級數 不用記公式，考試直接把級數展開就可以了 Example : \\(\\sum_{k=1}^{\\infty}(a_k - a_{k+1}) = a_1 - \\lim_{k \\to \\infty}a_k\\)，(要在數列收斂的情況下) 幾何級數 如果\\(x&lt;1\\)，則\\(\\sum_{k=0}^{\\infty}x^k = \\frac{1}{1-x}\\) 3.2.3 交錯級數 正負號相間的級數極為交錯級數 看到交錯級數馬上想到 Dirichlet Test Dirichlet Test 假設\\(a_k\\)和\\(b_k\\)為實數數列，如果\\(a_k\\)的部分和有界且\\(b_k\\)單調遞減到0，則我們有\\(\\sum_{k=1}^{\\infty}a_kb_k\\)收斂 Example : 證明\\(\\sum_{k=1}^{\\infty}(-1)^k/\\log k\\)收斂 3.2.4 冪級數 甚麼叫做一個冪級數，當我們把無窮級數寫成多項式的形式\\(P(x) = \\sum a_kx_k\\)，就稱為冪級數，但是我們無法從這個形式得到更多的資訊，所以我們一般在數學上定義寫作: \\[ P(x) = \\sum_{k = 0}^{\\infty}a_k(x - x_0)^k \\] \\(x_0\\) 是一個固定的常數，可以當作冪級數的中心點，或是展開點 收斂半徑\\((R)\\)的定義為所有\\(x\\)使得範圍\\(|x-x_0|&lt;R\\)級數絕對收斂，\\(|x-x_0|&gt;R\\)級數發散 冪級數的好處是在收斂半徑內可以做為微分以及積分的運算 3.2.5 收斂半徑 用完以上檢驗法之後，我們可以知道級數和是收斂或發散的，如果是收斂的，我們會想知道他在那些地方收斂，級數和收斂的範圍稱為收斂半徑，那麼收斂半徑怎麼算呢?? 把Root test和Ratio test顛倒過來看就對了!! 我們有以下公式: \\[R = \\frac{1}{\\displaystyle\\limsup_{n \\to \\infty}\\sqrt[n]{|a_n|}} = \\frac{1}{\\displaystyle\\lim_{n \\to \\infty}|\\frac{a_{n+1}}{a_n}|}\\] 端點要自己驗證是否收斂  把端點值帶入級數判斷是否收斂 3.2.6 泰勒展開式定理 如果一個函數可以在中心點為\\(x_0\\)的附近寫成冪級數的樣子，他的形式一定是以下的表達式: \\[f(x) = \\sum_{k = 0}^{\\infty}\\frac{f^{(n)}(x_0)(x-x_0)^n}{n!}\\] 如果\\(x_0 = 0\\)，換句話說，我們在中心點為\\(0\\)的地方展開時，又稱為Maclaurin級數 泰勒餘項為\\(\\displaystyle R_n(x) = f(x) - \\sum_{k=0}^{n-1}\\frac{f^{(k)}(x_0)(x-x_0)^k}{k!}\\) Example : Find the Taylor expansion of \\(e^x,\\sin x,\\cos x, \\ln (1+x)\\) at the center \\(x_0 = 0\\), you should also calculate the radius of convergence 通常用來找展開式的方法有以下三種 1. 泰勒展開式，利用微分去找級數的規律 2. 用二項式或者等比公式 3. 拿已知的展開式去改 3.3 純量場 甚麼叫做一個向量場??我們先來講講純量函數(純量場) 簡單來說，就是賦予空間上每一個點一個值，例如\\(f(x,y,z) = x^2 + y^2 + z^2\\) :notebook_with_decorative_cover: 空間中的弧長變化率為\\(ds = \\sqrt{(dx)^2 + (dy^2) + (dz)^2}\\) :notebook_with_decorative_cover: 空間中曲線的單位切向量變化為\\(du = (\\frac{dx}{ds},\\frac{dy}{ds},\\frac{dz}{ds})\\) 3.3.1 方向導數 方向導數是純量場中\\(f(x,y,z)\\)沿著某個向量上的導數，物理意義是純量場沿著方向\\((u)\\)的順時變化率，數學上我們寫做\\(D_uf(x,y,z) = \\displaystyle\\lim_{h \\to 0}\\frac{f(x_0+hu_1,y_0+hu_2,z_0 + hu_3) - f(x,y,z)}{h}\\) 假設\\(f(x,y,z)\\)為空間中一個純量場，我們想知道純量場基於位移\\((ds)\\)的順時變化率\\((df)\\)，即求\\(\\frac{df}{ds} = \\frac{\\partial f}{\\partial x}\\frac{dx}{ds} + \\frac{\\partial f}{\\partial y}\\frac{dy}{ds} + \\frac{\\partial f}{\\partial z}\\frac{dz}{ds}\\)或是我可們可以寫成\\(\\nabla f \\cdot \\vec u\\)，其中\\(\\vec u\\)為曲線的單位切向量 Example : 純量場\\(f(x,y,z)=x^2+y^3-2\\)，在點\\((1,2,8)\\)沿著方向\\((\\frac{1}{2},\\frac{\\sqrt{3}}{2},0)\\)的方向導數 其實方向導數就是斜率的概念，只是他的方向可以是任何的，不僅僅是\\(x\\)和\\(y\\) 方向導數定義\\(df/ds\\) = 純量場的變化 / 位移(單位弧長) 3.3.2 梯度 梯度表示純量場的值變化最為劇烈的方向，注意到，其實就是使方向導數最大的那個方向，所以我們的問題是\\(\\max{\\nabla f\\cdot \\vec u}\\)，回想起國高中的內積定義，其實就是梯度向量\\(\\nabla f\\)與\\(\\max\\vec u\\)同向， 梯度的方向其實就是使得切線斜率最大的那個方向 3.4 向量場 向量場其實與純量場很相像 簡單來說，就是賦予空間上每一個點一個向量，下面為兩張範例圖 3.4.1 散度 空間中給定一個向量場\\(\\vec F = (F_1(x,y,z),F_2(x,y,z),F_3(x,y,z))\\)，散度的物理意義是向量場在某一點\\((P)\\)的單位體積淨流出量(net flux)，或是說這個向量場在\\(P\\)點的發散程度。想要理解散度，我們先來講講甚麼叫做淨流出量 流量的數學定義為 \\(\\iint_C\\vec F \\cdot NdA\\)，其中\\(C\\)為一個封閉迴路，\\(A\\)為\\(C\\)圍成的面積，\\(N\\)為面積\\(A\\)的單位法向量，然而\\(\\vec F\\)不一定都指向上，所以我們要算\\(F\\)投影到法向量上的值來表示流量，最後，我們再把每一個點得到的值加總起來就是向量場\\(\\vec F\\)在\\(A\\)的流量 不難想像向量場\\(\\vec F\\)在與法向量平行的時候會有最大的流量 有了流量的概念，我們可以來談談散度了，散度的數學定義如下: \\[div\\vec F(P) = \\lim_{\\Omega \\to 0}\\frac{1}{\\Omega}\\iint_C\\vec F\\cdot NdA = \\frac{\\partial F_1}{\\partial x} + \\frac{\\partial F_2}{\\partial y} + \\frac{\\partial F_3}{\\partial z} = \\nabla \\cdot \\vec F\\] 其中\\(\\Omega\\)為點\\(P\\)附近圍成的面積，不難看出散度代表著向量場在單點的流出強度 3.4.2 旋度 在進入旋度前，我們先來講講環流量，空間中的向量場\\(\\vec F\\)沿著曲線\\(C\\)環繞的流量就稱為環流量，我們有以下的數學表達式: \\[\\oint_C\\vec F\\cdot Tds\\] 其中\\(T\\)為切線方向的單位向量，而旋度的定義就是把面積無線縮小之後的環流量，有數學式: \\[curl\\vec F = \\lim_{A \\to 0}\\frac{1}{A}\\oint\\vec F \\cdot Tds = (\\frac{\\partial F_3}{\\partial y} - \\frac{\\partial F_2}{\\partial z},\\frac{\\partial F_1}{\\partial z} - \\frac{\\partial F_3}{\\partial x}, \\frac{\\partial F_2}{\\partial x} - \\frac{\\partial F_1}{\\partial y}) = \\nabla \\cdot \\vec F\\] 散度是把通量除以無限縮小的體積，可以想成對通量的微分 旋度是把環流量除以無限縮小的面積，可以想成對環流量的微分 通量是單位時間內通過某個曲面的流量，散度為通量強度 流量是單位時間內環繞某個曲線的流量，旋度為流量強度 3.5 線積分 線積分主要分為兩類，線積分的精神為 分割長方形 算長方形面積 取極限和 3.5.1 第一類線積分 主要為對弧長\\(L\\)的積分，數學上記做\\(\\int_Lf(x, y)ds\\)，有方向但沒做功 被積函數可以視作線的質量密度，物理意義為求曲線的質量 如果有參數式\\((x, y) = (x(t), y(t)), t \\in[t_0, t_1]\\)，則弧長為\\(\\int_Lds = \\int_{t_0}^{t_1}\\sqrt{(x&#39;(t))^2+(y&#39;(t))^2}dt\\) 一般形式(參數式)為\\(\\int_Lf(x,y)ds = \\int_{t_0}^{t_1}f(x(t),y(t))\\sqrt{(x&#39;(t))^2+(y&#39;(t))^2}dt\\) 線積分可以分段積分(兩線段需接在一起) Example : \\(\\int_Lxyds\\), where \\(L\\) is a line from \\((-3,-3)\\) to \\((0,0)\\) and curve \\(y = 4x^2\\) for \\(x \\in [0,2]\\) 3.5.2 第二類線積分 空間中力沿著曲線的做功 回想國中做功的功式為 \\(功(W) = \\langle位移(S),力(F)\\rangle\\)，在這裡也一樣，我們想要研究空間向量場中一個向量(力)沿著曲線做了多少功，因為我們要計算位移與力的內積，我們給定沿著曲線切線方向的一個單位向量\\(\\vec T\\)並把\\(\\vec F\\)投影到\\(\\vec T\\)上(\\(\\vec F \\cdot \\vec T\\))，數學上寫作\\(\\int_L\\vec F \\cdot \\vec T ds\\) 既然\\(\\vec T\\)為曲線的單位切向量，我們有\\(\\vec T = \\frac{dr}{|dr|}\\)，其中\\(|dr| = \\sqrt{(dx)^2 + (dy)^2 + (dz)^2} = ds\\)，所以\\(\\int_L\\vec F \\cdot \\vec T ds = \\int_L \\vec F \\cdot d\\vec r\\)，假設\\(\\vec F = (F_1(x,y,z), F_2(x,y,z), F_3(x,y,z))\\)，則我們有形式\\(\\int_L\\vec F \\cdot \\vec T ds = \\int_L F_1dx + F_2dy + F_3dz\\) 換成形式\\(\\int_L F_1dx + F_2dy + F_3dz\\)則回到第一類線積分處理 Example : Find \\(\\int_L(x^2 - y^2)dx+2xydy\\) where \\(L(x,y) = (t^2, t^3)\\) with \\(t \\in [0,1]\\) 一定先找到曲線的參數式再下手，永遠記得換成參數式後要乘以一個轉換倍數 3.5.3 特殊曲線之擺縣 一個半徑為\\(r\\)的圓球向前滾動軌跡所形成的曲線，我們有參數式\\((x,y) = (r(t-\\sin t), r(1-\\cos t))\\) 3.5.4 與路徑無關的線積分 假設\\(\\vec F = (F_1(x,y,z), F_2(x,y,z), F_3(x,y,z))\\) 甚麼叫做與路徑無關??，要是從A點到B點的積分值沿著不同曲線積分都相同，我們就說這是與路徑無關的線積分，什麼樣的曲線積分滿足與路徑無關呢?? 如果存在位勢函數\\(\\phi(x,y,z)\\)使得\\(\\frac{\\partial\\phi}{\\partial x} = F_1\\)，\\(\\frac{\\partial\\phi}{\\partial y} = F_2\\)，\\(\\frac{\\partial\\phi}{\\partial z} = F_3\\)，或是\\(\\nabla\\phi = \\vec F\\) 我們稱這樣的\\(\\vec F\\)為保守場 保守場下的線積分與路徑無關，只與起點與終點有關 簡易推導過程 :+1: 我們已知\\(x,y,z\\)為\\(t\\)的函數，根據鏈鎖率我們有\\(\\frac{d \\phi}{dt} = \\frac{\\partial \\phi}{\\partial x}\\frac{dx}{dt}+\\frac{\\partial \\phi}{\\partial y}\\frac{dy}{dt}+\\frac{\\partial \\phi}{\\partial z}\\frac{dz}{dt}\\) 所以\\(d\\phi = \\frac{\\partial \\phi}{\\partial x}dx + \\frac{\\partial \\phi}{\\partial y}dy + \\frac{\\partial \\phi}{\\partial z}dz = F_1dx + F_2dy + F_3dz\\) 最後\\(\\int_LF_1dx + F_2dy + F_3dz = \\int_Ld\\phi = \\phi(end) - \\phi(start)\\) 如果起點與終點是同一個點的話，線積分為\\(\\phi(end) - \\phi(start) = 0\\) 保守場的旋度為0 簡易推導過程 :+1: 假設\\(\\phi\\)為保守場\\(\\vec F\\)的位勢函數 計算\\(curl \\vec F = \\nabla \\times \\vec F\\)，其中\\(\\nabla = (\\frac{\\partial}{\\partial x},\\frac{\\partial}{\\partial y},\\frac{\\partial}{\\partial z})\\) 利用二次微分且連續的性質可以說明微分順序的調換性 結論 : \\(\\phi\\) 二次可微分且連續的話，保守場的旋度為0 3.6 格林定理 \\(C\\)為平面上一條封閉曲線，其圍成的區域稱為\\(D\\)，如果向量場\\(\\vec F = (M(x,y),N(x,y))\\)在\\(D\\)上有一階連續偏導數，且\\(D\\)的面積為\\(A\\)，則我們有: \\[\\iint_D\\frac{\\partial M}{\\partial y}-\\frac{\\partial N}{\\partial x}dA = \\oint_CMdx+Ndy\\] 不要看格林定理公式很長就不知所措，格林定理的核心概念就是告訴你，在曲線上的線積分可以換成圍成區域上的面積分 Example: Find \\(\\oint_C 2xy-x^2dx+(x+y^2)dy\\) where \\(C\\) is the boundary of the area enclose by parabola \\(y = x^2\\) and line \\(y = x\\) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
